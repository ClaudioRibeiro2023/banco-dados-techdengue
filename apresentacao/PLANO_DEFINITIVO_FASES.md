# üöÄ PLANO DEFINITIVO - FASES COMPLETAS

**Continua√ß√£o do PLANO_DEFINITIVO_DASHBOARD.md**

---

## üìä FASE 2: M√ìDULOS DE AN√ÅLISE (4h)

### 2.1 Performance Analyzer (1h)

```python
# dashboard/modules/performance_analyzer.py
"""
M√≥dulo de An√°lise de Performance
"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List
from dashboard.core.data_processor import data_processor
from dashboard.core.cache_manager import cache_manager

class PerformanceAnalyzer:
    """An√°lise completa de performance operacional"""
    
    @cache_manager.cached(ttl_minutes=5)
    def calculate_kpis(self, df: pd.DataFrame) -> Dict:
        """Calcula KPIs principais"""
        return {
            'total_registros': len(df),
            'pois_total': df['POIS'].sum() if 'POIS' in df.columns else 0,
            'hectares_total': df['HECTARES_MAPEADOS'].sum() if 'HECTARES_MAPEADOS' in df.columns else 0,
            'pois_medio': df['POIS'].mean() if 'POIS' in df.columns else 0,
            'densidade': self._calculate_density(df)
        }
    
    def _calculate_density(self, df: pd.DataFrame) -> float:
        """Calcula densidade POIs/hectare"""
        pois = df['POIS'].sum() if 'POIS' in df.columns else 0
        hectares = df['HECTARES_MAPEADOS'].sum() if 'HECTARES_MAPEADOS' in df.columns else 1
        return pois / hectares if hectares > 0 else 0
    
    def get_top_municipalities(self, df: pd.DataFrame, n: int = 15) -> pd.DataFrame:
        """Top N munic√≠pios por n√∫mero de interven√ß√µes"""
        col_codigo = self._identify_municipality_column(df)
        if not col_codigo:
            return pd.DataFrame()
        
        return df[col_codigo].value_counts().head(n).reset_index()
    
    def temporal_evolution(self, df: pd.DataFrame) -> Dict:
        """An√°lise de evolu√ß√£o temporal"""
        if 'DATA_MAP' not in df.columns:
            return {}
        
        df_temp = df.copy()
        df_temp['mes'] = pd.to_datetime(df_temp['DATA_MAP']).dt.to_period('M')
        
        evolution = df_temp.groupby('mes').agg({
            'POIS': 'sum',
            'HECTARES_MAPEADOS': 'sum'
        }).reset_index()
        
        return {
            'monthly': evolution.to_dict('records'),
            'trend': self._calculate_trend(evolution)
        }
    
    def _calculate_trend(self, df: pd.DataFrame) -> str:
        """Calcula tend√™ncia (crescente, decrescente, est√°vel)"""
        if len(df) < 2:
            return 'insuficiente'
        
        first_half = df.iloc[:len(df)//2]['POIS'].mean()
        second_half = df.iloc[len(df)//2:]['POIS'].mean()
        
        diff_pct = ((second_half - first_half) / first_half) * 100
        
        if diff_pct > 10:
            return 'crescente'
        elif diff_pct < -10:
            return 'decrescente'
        return 'est√°vel'
    
    def _identify_municipality_column(self, df: pd.DataFrame) -> str:
        """Identifica coluna de munic√≠pio"""
        for col in ['CODIGO IBGE', 'C√≥digo IBGE', 'codigo_ibge', 'Municipio']:
            if col in df.columns:
                return col
        return None

# Inst√¢ncia global
performance_analyzer = PerformanceAnalyzer()
```

### 2.2 Impact Analyzer (1h)

```python
# dashboard/modules/impact_analyzer.py
"""
M√≥dulo de An√°lise de Impacto Epidemiol√≥gico
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from scipy import stats
from dashboard.core.cache_manager import cache_manager
from loguru import logger

class ImpactAnalyzer:
    """An√°lise de impacto epidemiol√≥gico"""
    
    @cache_manager.cached(ttl_minutes=10)
    def before_after_analysis(
        self,
        df_dengue_before: pd.DataFrame,
        df_dengue_after: pd.DataFrame,
        municipios_cisarp: List[str]
    ) -> Dict:
        """An√°lise before-after de casos de dengue"""
        
        results = []
        
        for codigo in municipios_cisarp:
            casos_before = self._sum_cases(df_dengue_before, codigo)
            casos_after = self._sum_cases(df_dengue_after, codigo)
            
            if casos_before > 0:
                variacao_pct = ((casos_after - casos_before) / casos_before) * 100
                
                results.append({
                    'municipio': codigo,
                    'casos_antes': casos_before,
                    'casos_depois': casos_after,
                    'variacao_absoluta': casos_after - casos_before,
                    'variacao_percentual': variacao_pct,
                    'classificacao': self._classify_impact(variacao_pct)
                })
        
        df_results = pd.DataFrame(results)
        
        return {
            'individual': df_results.to_dict('records'),
            'aggregate': self._aggregate_statistics(df_results),
            'cases_success': self._identify_success_cases(df_results)
        }
    
    def _sum_cases(self, df: pd.DataFrame, codigo: str) -> int:
        """Soma casos para munic√≠pio"""
        mun_data = df[df['codmun'].astype(str) == str(codigo)]
        if len(mun_data) == 0:
            return 0
        
        # Somar todas as colunas de semana
        week_cols = [col for col in df.columns if col.startswith('Semana')]
        return int(mun_data[week_cols].sum(axis=1).values[0]) if week_cols else 0
    
    def _classify_impact(self, variacao_pct: float) -> str:
        """Classifica impacto"""
        if variacao_pct < -20:
            return 'ALTA REDU√á√ÉO ‚≠ê‚≠ê‚≠ê'
        elif variacao_pct < -10:
            return 'REDU√á√ÉO MODERADA ‚≠ê‚≠ê'
        elif variacao_pct < 0:
            return 'REDU√á√ÉO LEVE ‚≠ê'
        return 'SEM REDU√á√ÉO'
    
    def _aggregate_statistics(self, df: pd.DataFrame) -> Dict:
        """Estat√≠sticas agregadas"""
        return {
            'total_municipios': len(df),
            'casos_antes_total': int(df['casos_antes'].sum()),
            'casos_depois_total': int(df['casos_depois'].sum()),
            'variacao_media': float(df['variacao_percentual'].mean()),
            'variacao_mediana': float(df['variacao_percentual'].median()),
            'municipios_com_reducao': int((df['variacao_percentual'] < 0).sum())
        }
    
    def _identify_success_cases(self, df: pd.DataFrame, threshold: float = -15) -> List[Dict]:
        """Identifica cases de sucesso"""
        success = df[df['variacao_percentual'] < threshold]
        return success.nlargest(5, 'variacao_percentual', keep='first').to_dict('records')
    
    def correlation_analysis(
        self,
        df_activities: pd.DataFrame,
        df_impact: pd.DataFrame
    ) -> Dict:
        """An√°lise de correla√ß√£o entre atividades e impacto"""
        
        # Merge datasets
        merged = pd.merge(
            df_activities.groupby('municipio').agg({'POIS': 'sum', 'HECTARES_MAPEADOS': 'sum'}),
            df_impact[['municipio', 'variacao_percentual']],
            on='municipio',
            how='inner'
        )
        
        # Correla√ß√£o de Pearson
        if len(merged) > 2:
            corr_pois, p_pois = stats.pearsonr(merged['POIS'], merged['variacao_percentual'])
            corr_hectares, p_hectares = stats.pearsonr(merged['HECTARES_MAPEADOS'], merged['variacao_percentual'])
            
            return {
                'correlation_pois': {
                    'coefficient': float(corr_pois),
                    'p_value': float(p_pois),
                    'significant': p_pois < 0.05
                },
                'correlation_hectares': {
                    'coefficient': float(corr_hectares),
                    'p_value': float(p_hectares),
                    'significant': p_hectares < 0.05
                },
                'interpretation': self._interpret_correlation(corr_pois, p_pois)
            }
        
        return {}
    
    def _interpret_correlation(self, corr: float, p_value: float) -> str:
        """Interpreta correla√ß√£o"""
        if p_value >= 0.05:
            return "Sem correla√ß√£o estatisticamente significativa"
        
        if corr < -0.5:
            return "Forte correla√ß√£o NEGATIVA (mais POIs = maior redu√ß√£o) ‚úÖ"
        elif corr < -0.3:
            return "Moderada correla√ß√£o negativa"
        elif corr < 0:
            return "Fraca correla√ß√£o negativa"
        return "Correla√ß√£o positiva ou nula"

# Inst√¢ncia global
impact_analyzer = ImpactAnalyzer()
```

### 2.3 Benchmark Analyzer (45min)

```python
# dashboard/modules/benchmark_analyzer.py
"""
M√≥dulo de Benchmarking
"""

import pandas as pd
from typing import Dict, List

class BenchmarkAnalyzer:
    """An√°lise de benchmarking e posicionamento"""
    
    def rank_contractors(self, df: pd.DataFrame, contractor_name: str = 'CISARP') -> Dict:
        """Ranking de contratantes"""
        
        ranking = df.groupby('CONTRATANTE').size().reset_index(name='atividades')
        ranking = ranking.sort_values('atividades', ascending=False)
        ranking['posicao'] = range(1, len(ranking) + 1)
        
        # Posi√ß√£o do CISARP
        cisarp_pos = ranking[ranking['CONTRATANTE'] == contractor_name]
        
        return {
            'ranking': ranking.head(10).to_dict('records'),
            'cisarp_position': int(cisarp_pos['posicao'].values[0]) if len(cisarp_pos) > 0 else 0,
            'total_contractors': len(ranking),
            'cisarp_percentile': self._calculate_percentile(cisarp_pos, ranking)
        }
    
    def _calculate_percentile(self, cisarp: pd.DataFrame, ranking: pd.DataFrame) -> float:
        """Calcula percentil do CISARP"""
        if len(cisarp) == 0:
            return 0.0
        
        pos = cisarp['posicao'].values[0]
        total = len(ranking)
        
        return (pos / total) * 100
    
    def compare_metrics(
        self,
        df: pd.DataFrame,
        contractor: str,
        comparison_group: List[str]
    ) -> Dict:
        """Compara m√©tricas entre grupos"""
        
        # M√©tricas do contratante
        contractor_data = df[df['CONTRATANTE'] == contractor]
        
        # M√©tricas do grupo de compara√ß√£o
        group_data = df[df['CONTRATANTE'].isin(comparison_group)]
        
        metrics = {}
        for col in ['POIS', 'HECTARES_MAPEADOS']:
            if col in df.columns:
                metrics[col] = {
                    'contractor_mean': float(contractor_data[col].mean()),
                    'group_mean': float(group_data[col].mean()),
                    'difference_pct': self._calc_diff_pct(
                        contractor_data[col].mean(),
                        group_data[col].mean()
                    )
                }
        
        return metrics
    
    def _calc_diff_pct(self, value: float, baseline: float) -> float:
        """Calcula diferen√ßa percentual"""
        if baseline == 0:
            return 0.0
        return ((value - baseline) / baseline) * 100

# Inst√¢ncia global
benchmark_analyzer = BenchmarkAnalyzer()
```

### 2.4 Insights Generator (45min)

```python
# dashboard/modules/insights_generator.py
"""
Gerador de Insights Inteligente
"""

from typing import List, Dict
import pandas as pd

class InsightsGenerator:
    """Gera√ß√£o autom√°tica de insights"""
    
    def generate_insights(
        self,
        kpis: Dict,
        impact: Dict,
        benchmark: Dict
    ) -> List[Dict]:
        """Gera insights baseados em m√∫ltiplas fontes"""
        
        insights = []
        
        # Insight 1: Performance
        insights.append({
            'category': 'performance',
            'title': f"üèÜ {benchmark.get('cisarp_position', 0)}¬∫ Lugar Nacional",
            'description': f"CISARP alcan√ßou {benchmark.get('cisarp_position', 0)}¬∫ lugar entre {benchmark.get('total_contractors', 0)} contratantes, posicionando-se no Top {benchmark.get('cisarp_percentile', 0):.0f}%",
            'metric': f"{benchmark.get('cisarp_position', 0)}¬∫/{benchmark.get('total_contractors', 0)}",
            'severity': 'success'
        })
        
        # Insight 2: Cobertura
        insights.append({
            'category': 'coverage',
            'title': "üìä Cobertura Territorial Abrangente",
            'description': f"{kpis.get('hectares_total', 0):,.0f} hectares mapeados com {kpis.get('pois_total', 0):,} POIs identificados",
            'metric': f"{kpis.get('hectares_total', 0):,.0f} ha",
            'severity': 'info'
        })
        
        # Insight 3: Impacto (se dispon√≠vel)
        if impact:
            avg_impact = impact.get('aggregate', {}).get('variacao_media', 0)
            if avg_impact < 0:
                insights.append({
                    'category': 'impact',
                    'title': "üíä Impacto Epidemiol√≥gico Positivo",
                    'description': f"Redu√ß√£o m√©dia de {abs(avg_impact):.1f}% nos casos de dengue em munic√≠pios com interven√ß√µes",
                    'metric': f"{avg_impact:.1f}%",
                    'severity': 'success'
                })
        
        # Insight 4: Densidade
        densidade = kpis.get('densidade', 0)
        insights.append({
            'category': 'efficiency',
            'title': "üîç Alta Densidade Operacional",
            'description': f"Densidade m√©dia de {densidade:.2f} POIs por hectare, demonstrando cobertura intensiva",
            'metric': f"{densidade:.2f} POIs/ha",
            'severity': 'info'
        })
        
        # Insight 5: Potencial
        gap_to_3rd = 3 - benchmark.get('cisarp_position', 4)
        if gap_to_3rd > 0:
            insights.append({
                'category': 'potential',
                'title': "üìà Potencial de Crescimento",
                'description': f"Gap de apenas {gap_to_3rd} posi√ß√µes para alcan√ßar o Top 3 nacional",
                'metric': f"+{gap_to_3rd} posi√ß√µes",
                'severity': 'warning'
            })
        
        return insights
    
    def generate_recommendations(self, insights: List[Dict]) -> Dict[str, List[str]]:
        """Gera recomenda√ß√µes baseadas em insights"""
        
        recommendations = {
            'curto_prazo': [],
            'medio_prazo': [],
            'longo_prazo': []
        }
        
        # An√°lise de insights para gerar recomenda√ß√µes
        for insight in insights:
            if insight['category'] == 'potential':
                recommendations['curto_prazo'].append(
                    "Expandir cobertura para munic√≠pios priorit√°rios"
                )
            elif insight['category'] == 'impact' and 'Positivo' in insight['title']:
                recommendations['medio_prazo'].append(
                    "Documentar cases de sucesso para replica√ß√£o"
                )
        
        # Recomenda√ß√µes padr√£o
        recommendations['curto_prazo'].extend([
            "Aumentar taxa de convers√£o de devolutivas",
            "Campanhas de conscientiza√ß√£o em √°reas de risco"
        ])
        
        recommendations['medio_prazo'].extend([
            "Implementar monitoramento cont√≠nuo",
            "Integrar com dados epidemiol√≥gicos em tempo real"
        ])
        
        recommendations['longo_prazo'].extend([
            "Estabelecer CISARP como modelo de refer√™ncia",
            "Desenvolver programa de capacita√ß√£o regional"
        ])
        
        return recommendations

# Inst√¢ncia global
insights_generator = InsightsGenerator()
```

**Entregas Fase 2:**
- ‚úÖ 4 M√≥dulos de an√°lise completos
- ‚úÖ Cache e otimiza√ß√£o integrados
- ‚úÖ Insights autom√°ticos
- ‚úÖ Recomenda√ß√µes baseadas em dados

---

## üì± FASE 3: P√ÅGINAS DO DASHBOARD (6h)

### Estrutura de P√°ginas

**3.1 Home (1h)** - Vis√£o executiva com KPIs principais  
**3.2 Performance (1.5h)** - An√°lise operacional detalhada  
**3.3 Impacto (1.5h)** - An√°lise epidemiol√≥gica completa  
**3.4 Benchmarking (1h)** - Compara√ß√µes e ranking  
**3.5 Explora√ß√£o (0.5h)** - Filtros e tabelas interativas  
**3.6 Insights (0.5h)** - Descobertas e recomenda√ß√µes  

*[Implementa√ß√£o detalhada dispon√≠vel mediante solicita√ß√£o]*

---

## üé® FASE 4: UI/UX E POLISH (3h)

### 4.1 Design System Aplicado (1h)
- Aplicar cores padronizadas em todos os componentes
- Implementar spacing consistente
- Adicionar anima√ß√µes suaves

### 4.2 Responsividade (1h)
- Mobile-first design
- Breakpoints otimizados
- Touch-friendly interface

### 4.3 Acessibilidade (1h)
- Alt text em gr√°ficos
- Contraste adequado
- Navega√ß√£o por teclado

---

## üß™ FASE 5: TESTES E QUALIDADE (3h)

### 5.1 Testes Unit√°rios (1h)
```python
# tests/test_data_processor.py
import pytest
from dashboard.core.data_processor import data_processor

def test_validate_dataframe():
    df = pd.DataFrame({'A': [1,2], 'B': [3,4]})
    assert data_processor.validate_dataframe(df, ['A', 'B'])

def test_safe_array():
    assert data_processor.safe_array(None) == []
    assert data_processor.safe_array([1,2,3]) == [1,2,3]
```

### 5.2 Testes de Integra√ß√£o (1h)
- Testar fluxo completo de dados
- Validar integra√ß√£o entre m√≥dulos
- Verificar cache e performance

### 5.3 Testes de UI (1h)
- Testar navega√ß√£o entre p√°ginas
- Validar responsividade
- Verificar exporta√ß√£o de dados

---

## üöÄ FASE 6: DEPLOY E DOCUMENTA√á√ÉO (2h)

### 6.1 Prepara√ß√£o para Deploy (1h)
```bash
# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements_dashboard_full.txt .
RUN pip install -r requirements_dashboard_full.txt
COPY dashboard/ ./dashboard/
COPY dados/ ./dados/
CMD ["streamlit", "run", "dashboard/app.py"]
```

### 6.2 Documenta√ß√£o Final (1h)
- README completo
- API documentation
- Guias de uso
- Troubleshooting

---

## ‚è±Ô∏è CRONOGRAMA CONSOLIDADO

| Fase | Dura√ß√£o | Complexidade | Prioridade |
|------|---------|--------------|------------|
| **0. Setup** | 1h | Baixa | üî¥ Cr√≠tica |
| **1. Core** | 3h | Alta | üî¥ Cr√≠tica |
| **2. M√≥dulos** | 4h | Alta | üî¥ Cr√≠tica |
| **3. P√°ginas** | 6h | M√©dia | üü° Alta |
| **4. UI/UX** | 3h | M√©dia | üü° Alta |
| **5. Testes** | 3h | M√©dia | üü¢ M√©dia |
| **6. Deploy** | 2h | Baixa | üü¢ M√©dia |
| **TOTAL** | **22h** | **Enterprise** | |

---

## ‚úÖ DIFERENCIAIS IMPLEMENTADOS

### vs Dashboard Anterior

| Aspecto | Anterior | NOVO (Definitivo) |
|---------|----------|-------------------|
| Arquitetura | Monol√≠tica | **Modular** ‚úÖ |
| Cache | B√°sico | **Inteligente c/ TTL** ‚úÖ |
| Valida√ß√£o | Simples | **Robusta c/ Pydantic** ‚úÖ |
| Design System | Inline | **Centralizado** ‚úÖ |
| Logging | Print | **Loguru estruturado** ‚úÖ |
| Testes | Nenhum | **Unit + Integration** ‚úÖ |
| Event Bus | N/A | **Cross-module** ‚úÖ |
| Performance | B√°sica | **Otimizada** ‚úÖ |

---

## üéØ RESULTADO FINAL

**Voc√™ ter√°:**

1. ‚úÖ Dashboard enterprise-grade profissional
2. ‚úÖ Arquitetura modular escal√°vel
3. ‚úÖ Sistema de cache inteligente
4. ‚úÖ Design system centralizado
5. ‚úÖ An√°lises autom√°ticas e insights
6. ‚úÖ Performance otimizada
7. ‚úÖ Testes automatizados
8. ‚úÖ Documenta√ß√£o completa

**Pronto para:**
- üìä Apresenta√ß√µes de alto impacto
- üîÑ Expans√£o futura
- üöÄ Deploy em produ√ß√£o
- üë• Compartilhamento com stakeholders

---

**Este √© o PLANO DEFINITIVO baseado em arquitetura enterprise comprovada do SIVEPI.**

**Pr√≥xima a√ß√£o:** Executar Fase 0 (Setup)
